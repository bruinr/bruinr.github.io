---
title: "9.6.2 - Support Vector Machine"
author: "Team 6"
date: "3/8/2021"
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(e1071)
```

### **9.6.2 - Support Vector Machine**
In order to fit an SVM using a non-linear kernel, we once again use the svm() function. However, now we use a different value of the parameter kernel. To fit an SVM with a polynomial kernel we use kernel="polynomial", and to fit an SVM model with a radial kernel we use kernel="radial". In the former case, we also use the degree arguement to specify a degree for the polynomial kernal (this is d in (9.22)), and in the latter case we use gamma to specify a value of Y for the radial basis kernel (9.24).
  We first generate some data with a non-linear class boundary, as follows:
```{r}
set.seed(1)
x = matrix(rnorm(200*2), ncol=2)
x[1:100, ] = x[1:100, ]+2
x[101:150, ] = x[101:150, ] -2
y = c(rep(1,150), rep(2,50))
dat = data.frame(x=x, y=as.factor(y))
```
  
  Plotting the data makes it clear that the class boundary is indeed non-linear:
```{r }
plot(x, col=y)
```



  The data is randomly split into training and testing groups. We then fit th training data using the svm() function with a radial kernel and Y = 1:

```{r}
train = sample(200,100)
svmfit = svm(y~., data=dat[train,], kernel="radial", gamma=1, cost = 1)
plot(svmfit, dat[train,])
```

The plot shows that the resulting SVM has a decidedly non-linear boundary. The summary() function can be used to obtain some information about the SVM fit:
```{r}
summary(svmfit)

```


We can see from the figure that there are a fair number of training errors in this SVM fit. If we increase the value of cost, we can reduce the number of training errors. However, this comes at the price of a more irregular decision boundary that seems to be at risk of overfitting the data.
```{r}
svmfit=svm(y~., data=dat[train, ], kernel='radial', gamma=1, cost=1e5)

plot(svmfit, dat[train, ])

```


We can perform cross-validation using tune() to select the best choice of Î³ and cost for an SVM with a radial kernel:

```{r}
set.seed(1)
tune.out = tune(svm, y~., data=dat[train, ], kernel="radial", ranges=list(cost=c(0.1,1,10,100,1000), gamma = c(0.5,1,2,3,4)))
summary(tune.out)
```


Therefore, the best choice of parameters involves cost=1 and gamma=2. We can view the test set predictions for this mdoel by applying the predict() functoin to the data. Notice that to do this we subset the dataframe dat using -train as an index set.
```{r}
table(true=dat[-train,"y"], pred=predict(tune.out$best.model, newdata=dat[-train,]))
```


10% of test observations are misclassified by this SVM
